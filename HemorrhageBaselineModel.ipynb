{"cells":[{"metadata":{},"cell_type":"markdown","source":"### ResNet50 Keras baseline model\n\nThis notebook takes you through some important steps in building a deep convnet in Keras for multilabel classification of brain CT scans. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport matplotlib.pyplot as plt\nimport collections\nfrom tqdm import tqdm_notebook as tqdm\nfrom datetime import datetime\n\nfrom math import ceil, floor\nimport cv2\n\nimport tensorflow as tf\nimport keras\n\nimport sys\n\nfrom keras_applications.resnet import ResNet50\n\nfrom sklearn.model_selection import ShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Helper functions\n\nFor windowing the input images (thanks to fellow competitors' notebooks\\*), and to normalize the pixel values between -1 and 1.\n\n\\* Source: https://www.kaggle.com/omission/eda-view-dicom-images-with-correct-windowing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def _get_first_of_dicom_field_as_int(x):\n    if type(x) == pydicom.multival.MultiValue:\n        return int(x[0])\n    else:\n        return int(x)\n\ndef _get_windowing(data):\n    dicom_fields = [data.WindowCenter, data.WindowWidth, data.RescaleSlope, data.RescaleIntercept]\n    return [_get_first_of_dicom_field_as_int(x) for x in dicom_fields]\n\n\ndef _window_image(img, window_center, window_width, slope, intercept):\n    img = (img * slope + intercept)\n    img_min = window_center - window_width//2\n    img_max = window_center + window_width//2\n    img[img<img_min] = img_min\n    img[img>img_max] = img_max\n    return img \n\ndef _normalize(img):\n    if img.max() == img.min():\n        return np.zeros(img.shape)\n    return 2 * (img - img.min())/(img.max() - img.min()) - 1\n\n\ndef _read(path, desired_size=(224, 224)):\n    \"\"\"Will be used in DataGenerator\"\"\"\n    \n    dcm = pydicom.dcmread(path)\n\n    window_params = _get_windowing(dcm) # (center, width, slope, intercept)\n\n    try:\n        # dcm.pixel_array might be corrupt (one case so far)\n        img = _window_image(dcm.pixel_array, *window_params)\n    except:\n        img = np.zeros(desired_size)\n\n    img = _normalize(img)\n\n    if desired_size != (512, 512):\n        # resize image\n        img = cv2.resize(img, desired_size, interpolation=cv2.INTER_LINEAR)\n\n    return img[:,:,np.newaxis]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Data generators\n\nYou could make this in to just one DataGenerator, but I kept it as two separate DataGenerators (one for training and one for predicting). It inherits from keras.utils.Sequence object and thus should be safe for multiprocessing.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataGenerator(keras.utils.Sequence):\n\n    def __init__(self, list_IDs, labels, batch_size=1, img_size=(512, 512), \n                 img_dir='../input/data/stage_1_train_images/', *args, **kwargs):\n\n        self.list_IDs = list_IDs\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indices]\n        X, Y = self.__data_generation(list_IDs_temp)\n        return X, Y\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.list_IDs))\n        np.random.shuffle(self.indices)\n\n    def __data_generation(self, list_IDs_temp):\n        X = np.empty((self.batch_size, *self.img_size, 1))\n        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n        \n        for i, ID in enumerate(list_IDs_temp):\n            X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n            Y[i,] = self.labels.loc[ID].values\n        \n        return X, Y\n    \n    \nclass TestDataGenerator(keras.utils.Sequence):\n\n    def __init__(self, list_IDs, labels, batch_size=1, img_size=(512, 512), \n                 img_dir='../input/data/stage_1_test_images/', *args, **kwargs):\n\n        self.list_IDs = list_IDs\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indices]\n        X = self.__data_generation(list_IDs_temp)\n        return X\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.list_IDs))\n\n    def __data_generation(self, list_IDs_temp):\n        X = np.empty((self.batch_size, *self.img_size, 1))\n        \n        for i, ID in enumerate(list_IDs_temp):\n            X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Model\n\nBasically a combination of three models, which are sequentially concatenated. <br> \n\n* The initial layer, which will transform/map input image of shape (\\_, \\_, 1) to another \"image\" of shape (\\_, \\_, 3).\n\n* The new input image is then passed through ResNet50 (which I named \"engine\"). ResNet50 could be replaced by any of the available architectures in keras_application.\n\n* Finally, the output from ResNet50 goes through average pooling followed by a dense output layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"def _initial_layer(input_dims):\n    inputs = keras.layers.Input(input_dims)\n    \n    x = keras.layers.Conv2D(filters=3, kernel_size=(1, 1), strides=(1, 1), name=\"initial_conv2d\")(inputs)\n    x = keras.layers.BatchNormalization(axis=3, epsilon=1.001e-5, name='initial_bn')(x)\n    x = keras.layers.Activation('relu', name='initial_relu')(x)\n    \n    return keras.models.Model(inputs, x)\n\nclass MyDeepModel:\n    \n    def __init__(self, engine, input_dims, batch_size=5, learning_rate=1e-3, \n                 decay_rate=1.0, decay_steps=1, weights=\"imagenet\", verbose=1):\n        \n        self.engine = engine\n        self.input_dims = input_dims\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.decay_rate = decay_rate\n        self.decay_steps = decay_steps\n        self.weights = weights\n        self.verbose = verbose\n        self._build()\n\n    def _build(self):\n        \n        initial_layer = _initial_layer((*self.input_dims, 1))\n    \n        engine = self.engine(include_top=False, weights=self.weights, input_shape=(*self.input_dims, 3),\n                             backend = keras.backend, layers = keras.layers,\n                             models = keras.models, utils = keras.utils)\n\n        x = engine(initial_layer.output)\n\n        x = keras.layers.GlobalAveragePooling2D(name='avg_pool')(x)\n\n        out = keras.layers.Dense(6, activation=\"sigmoid\", name='dense_output')(x)\n\n        self.model = keras.models.Model(inputs=initial_layer.input, outputs=out)\n\n        self.model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(0.0))\n    \n    \n    def fit(self, df, train_idx, img_dir, global_epoch):\n        self.model.fit_generator(\n            TrainDataGenerator(\n                df.iloc[train_idx].index, \n                df.iloc[train_idx], \n                self.batch_size, \n                self.input_dims, \n                img_dir\n            ),\n            verbose=self.verbose,\n            use_multiprocessing=True,\n            workers=4,\n            callbacks=[\n                keras.callbacks.LearningRateScheduler(\n                    lambda epoch: self.learning_rate * pow(self.decay_rate, floor(global_epoch / self.decay_steps))\n                )\n            ]\n        )\n    \n    def predict(self, df, test_idx, img_dir):\n        predictions = \\\n          self.model.predict_generator(\n            TestDataGenerator(\n                df.iloc[test_idx].index, \n                None, \n                self.batch_size, \n                self.input_dims, \n                img_dir\n            ),\n            verbose=1,\n            use_multiprocessing=True,\n            workers=4\n        )\n\n        return predictions[:df.iloc[test_idx].shape[0]]\n    \n    def save(self, path):\n        self.model.save_weights(path)\n    \n    def load(self, path):\n        self.model.load_weights(path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Read csv files\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_testset(filename=\"../input/rsna-intracranial-hemorrhage-detection/stage_1_sample_submission.csv\"):\n    df = pd.read_csv(filename)\n    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n    \n    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n    \n    return df\n\ndef read_trainset(filename=\"../input/rsna-intracranial-hemorrhage-detection/stage_1_train.csv\"):\n    df = pd.read_csv(filename)\n    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n    \n    duplicates_to_remove = [\n        1598538, 1598539, 1598540, 1598541, 1598542, 1598543,\n        312468,  312469,  312470,  312471,  312472,  312473,\n        2708700, 2708701, 2708702, 2708703, 2708704, 2708705,\n        3032994, 3032995, 3032996, 3032997, 3032998, 3032999\n    ]\n    \n    df = df.drop(index=duplicates_to_remove)\n    df = df.reset_index(drop=True)\n    \n    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n    \n    return df\n\n    \ntest_df = read_testset()\ndf = read_trainset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Train model and predict\n\n*Using train, validation and test set* <br>\n\nTraining for 3 epochs with Adam optimizer. The learning rate starts at 0.001, with a slight decay (0.75) each epoch. The predictions are then \\[exponentially weighted\\] averaged over all 3 epochs. Same goes for the test set submission later."},{"metadata":{"trusted":true},"cell_type":"code","source":"_TEST_IMAGES = '../input/rsna-intracranial-hemorrhage-detection/stage_1_test_images/'\n_TRAIN_IMAGES = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/'\n\ndef run(model, df, train_idx, valid_idx, test_df, epochs):\n    \n    valid_predictions = []\n    test_predictions = []\n    for global_epoch in range(epochs):\n\n        model.fit(df, train_idx, _TRAIN_IMAGES, global_epoch)\n        \n        test_predictions.append(model.predict(test_df, range(test_df.shape[0]), _TEST_IMAGES))\n        valid_predictions.append(model.predict(df, valid_idx, _TRAIN_IMAGES))\n        \n        print(\"validation loss: %.4f\" %\n              weighted_loss_metric(df.iloc[valid_idx].values, \n                                   np.average(valid_predictions, axis=0, \n                                              weights=[2**i for i in range(len(valid_predictions))]))\n             )\n    \n    return test_predictions, valid_predictions\n\n\n\ndef weighted_loss_metric(trues, preds, weights=[0.2, 0.1, 0.1, 0.1, 0.1, 0.1], clip_value=1e-7):\n    \"\"\"this is probably not correct, but works OK. Feel free to give feedback.\"\"\"\n    preds = np.clip(preds, clip_value, 1-clip_value)\n    loss_subtypes = trues * np.log(preds) + (1 - trues) * np.log(1 - preds)\n    loss_weighted = np.average(loss_subtypes, axis=1, weights=weights)\n    return - loss_weighted.mean()\n\n# train set (90%) and validation set (10%)\nss = ShuffleSplit(n_splits=5, test_size=0.1, random_state=42).split(df.index)\n\n# will just do one fold\ntrain_idx, valid_idx = next(ss)\n\n# obtain model\nmodel = MyDeepModel(engine=ResNet50, input_dims=(224, 224), batch_size=32, learning_rate=1e-3, \n                    decay_rate=0.75, decay_steps=1, weights=\"imagenet\", verbose=2)\n\n# run 3 epochs and obtain test + validation predictions\ntest_preds, _ = run(model, df, train_idx, valid_idx, test_df, 3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Submit test predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.iloc[:, :] = np.average(test_preds, axis=0, weights=[2**i for i in range(len(test_preds))])\n\ntest_df = test_df.stack().reset_index()\n\ntest_df.insert(loc=0, column='ID', value=test_df['Image'].astype(str) + \"_\" + test_df['Diagnosis'])\n\ntest_df = test_df.drop([\"Image\", \"Diagnosis\"], axis=1)\n\ntest_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Improvements\n\nSome improvements that could be made:<br>\n* Image augmentation (which can be put in `_read()`)\n* Different learning rate and learning rate schedule\n* Increased input size\n* Train longer (or shorter? :O)\n* Add regularization (e.g. `keras.layers.Dropout()` before the output layer)\n* Do something about `_initial_layer()`?\n<br>\n<br>\n*Feel free to comment!*\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}